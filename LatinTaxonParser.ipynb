{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1D85wbv-zo-KvioT1UbAoGpl7UqWtjy2z",
      "authorship_tag": "ABX9TyOaPoFRS5POBVLA+YdI6N2k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eliconger/eliconger/blob/main/LatinTaxonParser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Program to find Latin words within compound taxon names\n",
        "# using a dictionary of known Latin words and a Trie for fast substring lookup\n",
        "\n",
        "# Importing necessary modules\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import pandas as pd  # Import pandas for reading Excel files\n",
        "import numpy as np   # To handle NaN values robustly\n",
        "\n",
        "# Define a class for the Trie Node\n",
        "class TrieNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}              # Dictionary to store child nodes\n",
        "        self.is_end_of_word = False     # Flag to indicate if node marks end of a valid word\n",
        "\n",
        "# Define a class for the Trie structure\n",
        "class Trie:\n",
        "    def __init__(self):\n",
        "        self.root = TrieNode()         # Initialize root node\n",
        "\n",
        "    # Insert a word into the trie\n",
        "    def insert(self, word):\n",
        "        node = self.root\n",
        "        for char in word:              # Loop through each character in the word\n",
        "            if char not in node.children:\n",
        "                node.children[char] = TrieNode()  # Create a new node if character not found\n",
        "            node = node.children[char]            # Move to the child node\n",
        "        node.is_end_of_word = True     # Mark end of word\n",
        "\n",
        "    # Search for the longest substrings in a given text that are in the Trie\n",
        "    def search_substrings(self, text):\n",
        "        matches = []                                 # List to store all matches found\n",
        "        n = len(text)\n",
        "        for i in range(n):                           # Loop through each character index in the text\n",
        "            node = self.root\n",
        "            longest_match = None                     # To keep track of longest valid word match\n",
        "            j = i\n",
        "            while j < n and text[j] in node.children:\n",
        "                node = node.children[text[j]]         # Move to next character node\n",
        "                j += 1\n",
        "                if node.is_end_of_word:\n",
        "                    longest_match = (i, j, text[i:j]) # Update longest match if end of word reached\n",
        "            if longest_match:\n",
        "                matches.append(longest_match)        # Add only the longest match found at this start\n",
        "        return matches\n",
        "\n",
        "# Function to build a Trie from a list of Latin words\n",
        "# Also performs optional morphological stemming of Latin word endings\n",
        "\n",
        "def build_trie(latin_words):\n",
        "    suffixes = [\"us\", \"um\", \"a\", \"is\", \"os\", \"es\", \"ata\", \"atus\", \"atae\", \"ae\", \"ior\"]\n",
        "    trie = Trie()                                  # Initialize a Trie\n",
        "    for word in latin_words:\n",
        "        word = word.lower()                        # Convert to lowercase\n",
        "        trie.insert(word)                          # Insert full word\n",
        "        for suffix in suffixes:\n",
        "            if word.endswith(suffix):\n",
        "                stem = word[:-len(suffix)]         # Strip suffix to get potential stem\n",
        "                if len(stem) > 2:                  # Only insert reasonable-length stems\n",
        "                    trie.insert(stem)              # Insert stem into the Trie\n",
        "    return trie\n",
        "\n",
        "# Function to tokenize a taxon using Trie matches\n",
        "# This version also uses scoring logic to prefer longer matches and avoid overlaps\n",
        "\n",
        "def tokenize_taxon(taxon, trie):\n",
        "    taxon = taxon.lower()                         # Normalize to lowercase\n",
        "    matches = trie.search_substrings(taxon)       # Find all matches using the Trie\n",
        "    matches.sort(key=lambda x: (x[0], -(x[1] - x[0])))  # Sort by start index, then longest match\n",
        "\n",
        "    tokens = []\n",
        "    used = [False] * len(taxon)                   # Track used indices to avoid overlaps\n",
        "\n",
        "    for start, end, word in matches:\n",
        "        if not any(used[start:end]):              # Only accept if no overlap with previous matches\n",
        "            tokens.append(word)                   # Add the word to the result\n",
        "            for i in range(start, end):\n",
        "                used[i] = True                    # Mark indices as used\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Function to count occurrences of Latin roots from parsed CSV file\n",
        "\n",
        "def count_latin_root_occurrences(csv_path):\n",
        "    df = pd.read_csv(csv_path)                    # Load parsed results from CSV\n",
        "    root_counts = defaultdict(int)                # Dictionary to count roots\n",
        "\n",
        "    for roots in df[\"Latin Roots\"]:\n",
        "        if isinstance(roots, str):                # Ensure value is a string before splitting\n",
        "            for root in roots.split(\",\"):\n",
        "                cleaned_root = root.strip()\n",
        "                if cleaned_root:\n",
        "                    root_counts[cleaned_root] += 1     # Increment count for each root\n",
        "\n",
        "    count_df = pd.DataFrame(\n",
        "        list(root_counts.items()),\n",
        "        columns=[\"Latin Root\", \"Count\"]\n",
        "    ).sort_values(by=\"Count\", ascending=False)\n",
        "\n",
        "    count_df.to_csv(\"latin_root_occurrences.csv\", index=False)  # Save to file\n",
        "    print(\"Latin root occurrences saved to 'latin_root_occurrences.csv'\")\n",
        "    return count_df\n",
        "\n",
        "# Main program\n",
        "if __name__ == \"__main__\":\n",
        "    # Ask user for file paths\n",
        "    latin_path = input(\"Enter path to Latin words text file: \")  # Path to .txt file (one word per line)\n",
        "    taxon_path = input(\"Enter path to Excel file with taxons: \")  # Path to .xlsx file\n",
        "\n",
        "    # Read Latin words from file (one per line)\n",
        "    with open(latin_path, 'r', encoding='utf-8') as f:\n",
        "        latin_words = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    # Read taxons from Excel file, assume second column (index 1)\n",
        "    df = pd.read_excel(taxon_path)               # Read Excel file into DataFrame\n",
        "    taxons = df.iloc[:, 1].dropna().astype(str).tolist()  # Get SECOND column, drop NaNs, convert to strings\n",
        "    #taxons = df.iloc[:, 0].dropna().astype(str).tolist() #FIRST COLUMN CODE\n",
        "\n",
        "\n",
        "    # Build the Trie from Latin dictionary with stem variations\n",
        "    trie = build_trie(latin_words)\n",
        "\n",
        "    # Parse each taxon and collect results\n",
        "    output_data = []\n",
        "    for taxon in taxons:\n",
        "        parts = tokenize_taxon(taxon, trie)             # Tokenize taxon name\n",
        "        output_data.append({\"Taxon\": taxon, \"Latin Roots\": \", \".join(parts)})\n",
        "\n",
        "    # Save parsed tokens to CSV\n",
        "    output_df = pd.DataFrame(output_data)\n",
        "    output_df.to_csv(\"parsed_taxon_roots.csv\", index=False)\n",
        "    print(\"Parsed taxon roots saved to 'parsed_taxon_roots.csv'\")\n",
        "\n",
        "    # Count and save Latin root occurrences\n",
        "    count_latin_root_occurrences(\"parsed_taxon_roots.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "OCMPK4dK5jta",
        "outputId": "98445010-f89a-4af1-d307-a348e1c5e263"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter path to Latin words text file: /content/extracted_latin_words.txt\n",
            "Enter path to Excel file with taxons: /content/extracted_words.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Excel file format cannot be determined, you must specify an engine manually.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-132f243550ca>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# Read taxons from Excel file, assume second column (index 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxon_path\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# Read Excel file into DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;31m#taxons = df.iloc[:, 1].dropna().astype(str).tolist()  # Get SECOND column, drop NaNs, convert to strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mtaxons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#FIRST COLUMN CODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n\u001b[1;32m   1553\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m   1555\u001b[0m                         \u001b[0;34m\"Excel file format cannot be determined, you must specify \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m                         \u001b[0;34m\"an engine manually.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Excel file format cannot be determined, you must specify an engine manually."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to extract second word in taxons\n",
        "import pandas as pd\n",
        "\n",
        "def extract_second_words(df):\n",
        "    first_col = df.columns[0]\n",
        "    words = df[first_col].apply(\n",
        "        lambda x: x.split()[1] if isinstance(x, str) and len(x.split()) > 1 else None\n",
        "    )\n",
        "    return pd.DataFrame({'SecondWord': words.dropna()})\n",
        "\n",
        "def main():\n",
        "    file_path = input(\"Enter the path to your Excel file: \").strip()\n",
        "\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "        second_words_df = extract_second_words(df)\n",
        "\n",
        "        output_file = \"extracted_words.csv\"\n",
        "        second_words_df.to_csv(output_file, index=False)\n",
        "\n",
        "        print(f\"\\nExtraction complete. Words saved to '{output_file}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "5ZJnWWjl5rPd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "297d7e87-e2ac-40f6-c6e2-971f6fc28f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path to your Excel file: /content/Latin Taxons, etym starts with\"Named after Latin\" .xlsx\n",
            "\n",
            "Extraction complete. Words saved to 'extracted_words.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next code is the original Trie implementation without backtracking capabilities"
      ],
      "metadata": {
        "id": "rYJPoBxX2M9A"
      }
    }
  ]
}